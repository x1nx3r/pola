
import json
import os

def update_notebook():
    input_nb = 'pola_svm_classification.ipynb'
    output_nb = 'pola_svm_nca9.ipynb'
    
    if not os.path.exists(input_nb):
        print(f"Error: {input_nb} not found.")
        return

    with open(input_nb, 'r') as f:
        nb = json.load(f)

    # 1. Update Step 1 Configuration (Cell 2 usually, check source)
    for cell in nb['cells']:
        if cell['cell_type'] == 'code':
            source = "".join(cell['source'])
            if "# Configuration" in source:
                new_source = [
                    "print(\"=\"*70)\n",
                    "print(\"STEP 1: DATA PREPARATION (NCA 9)\")\n",
                    "print(\"=\"*70)\n",
                    "\n",
                    "# Configuration\n",
                    "features_file = '../data/features_nca_9.csv'\n",
                    "\n",
                    "if os.path.exists(features_file):\n",
                    "    print(f\"Using 9-component NCA features: {features_file}\")\n",
                    "else:\n",
                    "    raise FileNotFoundError(f\"Feature file not found: {features_file}\")\n",
                    "\n",
                    "# Load the features dataset\n",
                    "df = pd.read_csv(features_file)\n",
                    "\n",
                    "print(f\"\\nLoaded dataset: {features_file}\")\n",
                    "print(f\"Dataset shape: {df.shape}\")\n",
                    "\n",
                    "# Extract Features (columns starting with 'nca')\n",
                    "feature_cols = [c for c in df.columns if c.startswith('nca')]\n",
                    "X = df[feature_cols].values\n",
                    "feature_names = feature_cols\n",
                    "\n",
                    "# Extract Targets (robust label extraction)\n",
                    "if 'label' in df.columns:\n",
                    "    y_raw = df['label'].values\n",
                    "    print(\"Using 'label' column for targets\")\n",
                    "elif 'label_code' in df.columns:\n",
                    "    y_raw = df['label_code'].values\n",
                    "    print(\"Using 'label_code' column for targets\")\n",
                    "else:\n",
                    "    # Fallback to filename extraction\n",
                    "    print(\"Extracting labels from filenames...\")\n",
                    "    y_raw = df['filename'].astype(str).str.extract(r'(?i)(h\\d+)')[0].str.upper().fillna('UNKNOWN').values\n",
                    "\n",
                    "print(f\"Number of species: {len(np.unique(y_raw))}\")\n",
                    "print(f\"\\nSpecies distribution:\")\n",
                    "print(pd.Series(y_raw).value_counts())\n",
                    "\n",
                    "# Handle any NaN values\n",
                    "if np.isnan(X).any():\n",
                    "    print(f\"\\nWarning: Found {np.isnan(X).sum()} NaN values. Filling with column means.\")\n",
                    "    col_mean = np.nanmean(X, axis=0)\n",
                    "    inds = np.where(np.isnan(X))\n",
                    "    X[inds] = np.take(col_mean, inds[1])\n",
                    "\n",
                    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
                    "print(f\"Number of features: {len(feature_names)}\")\n",
                    "print(f\"Number of samples: {len(y_raw)}\")\n",
                    "\n",
                    "# Encode species labels\n",
                    "le = LabelEncoder()\n",
                    "y_encoded = le.fit_transform(y_raw)\n",
                    "species_names = le.classes_\n",
                    "\n",
                    "print(f\"\\nEncoded labels: {dict(zip(species_names, range(len(species_names))))}\")\n",
                    "\n",
                    "# Split data into train and test sets (80/20 split)\n",
                    "X_train, X_test, y_train, y_test = train_test_split(\n",
                    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
                    ")\n",
                    "\n",
                    "print(f\"\\nTrain set size: {len(X_train)} samples\")\n",
                    "print(f\"Test set size: {len(X_test)} samples\")\n",
                    "print(f\"Train set distribution:\")\n",
                    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
                    "for idx, count in train_dist.items():\n",
                    "    print(f\"  {species_names[idx]}: {count}\")\n",
                    "\n",
                    "# Standardize features (fit on train, transform both)\n",
                    "scaler_svm = StandardScaler()\n",
                    "X_train_scaled = scaler_svm.fit_transform(X_train)\n",
                    "X_test_scaled = scaler_svm.transform(X_test)\n",
                    "\n",
                    "print(\"\\n✓ Features standardized (zero mean, unit variance)\")\n",
                    "print(\"✓ Data preparation complete\")"
                ]
                cell['source'] = new_source
                print("Updated configuration cell.")

            # 2. Update Step 8 Suffix Logic
            if "if 'features_nca_10.csv' in features_file:" in source:
                new_source_8 = [
                    "print(\"\\n\" + \"=\"*70)\n",
                    "print(\"STEP 8: SAVE MODEL AND GENERATE REPORT\")\n",
                    "print(\"=\"*70)\n",
                    "\n",
                    "# Define filenames for NCA models\n",
                    "if 'features_nca_10.csv' in features_file:\n",
                    "    suffix = 'nca10'\n",
                    "elif 'features_nca_2.csv' in features_file:\n",
                    "    suffix = 'nca2'\n",
                    "elif 'features_nca_9.csv' in features_file:\n",
                    "    suffix = 'nca9'\n",
                    "else:\n",
                    "    suffix = 'nca'\n",
                    "\n",
                    "model_filename = f'../models/pola_{suffix}_svm_best.joblib'\n",
                    "scaler_filename = f'../models/pola_{suffix}_scaler.joblib'\n",
                    "label_encoder_filename = f'../models/pola_{suffix}_label_encoder.joblib'\n",
                    "summary_filename = f'../reports/pola_{suffix}_svm_summary.json'\n",
                    "\n",
                    "joblib.dump(best_svm, model_filename)\n",
                    "joblib.dump(scaler_svm, scaler_filename)\n",
                    "joblib.dump(le, label_encoder_filename)\n",
                    "\n",
                    "print(f\"\\nSaved model artifacts:\")\n",
                    "print(f\"  Model: {model_filename}\")\n",
                    "print(f\"  Scaler: {scaler_filename}\")\n",
                    "print(f\"  Label encoder: {label_encoder_filename}\")\n",
                    "\n",
                    "# Generate comprehensive summary report\n",
                    "summary_report = {\n",
                    "    'Model Type': f'Support Vector Machine (SVM) - {suffix.upper()}',\n",
                    "    'Best Kernel': best_svm.kernel,\n",
                    "    'Best Parameters': best_svm.get_params(),\n",
                    "    'Training Samples': len(X_train),\n",
                    "    'Test Samples': len(X_test),\n",
                    "    'Number of Features': X.shape[1],\n",
                    "    'Number of Classes': len(species_names),\n",
                    "    'Class Names': list(species_names),\n",
                    "    'Test Accuracy': f\"{accuracy:.4f}\",\n",
                    "    'Test Precision': f\"{precision:.4f}\",\n",
                    "    'Test Recall': f\"{recall:.4f}\",\n",
                    "    'Test F1-Score': f\"{f1:.4f}\",\n",
                    "    'CV Mean Accuracy': f\"{cv_scores.mean():.4f}\",\n",
                    "    'CV Std Accuracy': f\"{cv_scores.std():.4f}\",\n",
                    "    'Number of Support Vectors': len(best_svm.support_),\n",
                    "    'Support Vector Ratio': f\"{len(best_svm.support_) / len(X_train_scaled):.2%}\",\n",
                    "}\n",
                    "\n",
                    "# Save summary to JSON\n",
                    "with open(summary_filename, 'w') as f:\n",
                    "    json.dump(summary_report, f, indent=2, default=str)\n",
                    "\n",
                    "print(f\"\\nSaved model summary: {summary_filename}\")\n",
                    "\n",
                    "# Print final summary\n",
                    "print(\"\\n\" + \"=\"*70)\n",
                    "print(\"SVM CLASSIFICATION PIPELINE - FINAL SUMMARY\")\n",
                    "print(\"=\"*70)\n",
                    "\n",
                    "print(f\"\\n{'Model Configuration:':<30}\")\n",
                    "print(f\"  {'Kernel:':<28} {best_svm.kernel}\")\n",
                    "if best_svm.kernel in ['rbf', 'poly']:\n",
                    "    print(f\"  {'C parameter:':<28} {best_svm.C}\")\n",
                    "    print(f\"  {'Gamma:':<28} {best_svm.gamma}\")\n",
                    "if best_svm.kernel == 'poly':\n",
                    "    print(f\"  {'Degree:':<28} {best_svm.degree}\")\n",
                    "elif best_svm.kernel == 'linear':\n",
                    "    print(f\"  {'C parameter:':<28} {best_svm.C}\")\n",
                    "\n",
                    "print(f\"\\n{'Dataset Information:':<30}\")\n",
                    "print(f\"  {'Total samples:':<28} {len(X)}\")\n",
                    "print(f\"  {'Training samples:':<28} {len(X_train)}\")\n",
                    "print(f\"  {'Test samples:':<28} {len(X_test)}\")\n",
                    "print(f\"  {'Number of features:':<28} {X.shape[1]}\")\n",
                    "print(f\"  {'Number of species:':<28} {len(species_names)}\")\n",
                    "\n",
                    "print(f\"\\n{'Performance Metrics:':<30}\")\n",
                    "print(f\"  {'Test Accuracy:':<28} {accuracy:.4f}\")\n",
                    "print(f\"  {'Test Precision:':<28} {precision:.4f}\")\n",
                    "print(f\"  {'Test Recall:':<28} {recall:.4f}\")\n",
                    "print(f\"  {'Test F1-Score:':<28} {f1:.4f}\")\n",
                    "print(f\"  {'CV Accuracy (10-fold):':<28} {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
                    "\n",
                    "print(f\"\\n{'Model Characteristics:':<30}\")\n",
                    "print(f\"  {'Support vectors:':<28} {len(best_svm.support_)}\")\n",
                    "print(f\"  {'Support vector ratio:':<28} {len(best_svm.support_) / len(X_train_scaled):.2%}\")\n",
                    "print(f\"  {'Misclassified (test):':<28} {n_misclassified}/{len(y_test)} ({n_misclassified/len(y_test)*100:.2f}%)\")\n",
                    "\n",
                    "print(f\"\\n{'Output Files:':<30}\")\n",
                    "print(f\"  • {model_filename}\")\n",
                    "print(f\"  • {scaler_filename}\")\n",
                    "print(f\"  • {label_encoder_filename}\")\n",
                    "print(f\"  • {summary_filename}\")\n",
                    "\n",
                    "print(\"\\n\" + \"=\"*70)\n",
                    "print(\"✓ SVM CLASSIFICATION PIPELINE COMPLETE\")\n",
                    "print(\"=\"*70)\n",
                    "\n",
                    "print(f\"\\nThe trained SVM model is ready.\")"
                ]
                cell['source'] = new_source_8
                print("Updated Setp 8.")

    with open(output_nb, 'w') as f:
        json.dump(nb, f, indent=2)
    print(f"Created {output_nb}")

if __name__ == "__main__":
    update_notebook()
