{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8650ea22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import time\n",
        "import json\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "import joblib\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(\"Ready to build SVM classification pipeline for NCA features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5546b2ef",
      "metadata": {},
      "source": [
        "## Step 1: Data Preparation\n",
        "\n",
        "Load the NCA features and prepare the dataset for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a56323",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"STEP 1: DATA PREPARATION (NCA 9)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Configuration\n",
        "features_file = '../data/features_nca_9.csv'\n",
        "\n",
        "if os.path.exists(features_file):\n",
        "    print(f\"Using 9-component NCA features: {features_file}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Feature file not found: {features_file}\")\n",
        "\n",
        "# Load the features dataset\n",
        "df = pd.read_csv(features_file)\n",
        "\n",
        "print(f\"\\nLoaded dataset: {features_file}\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "# Extract Features (columns starting with 'nca')\n",
        "feature_cols = [c for c in df.columns if c.startswith('nca')]\n",
        "X = df[feature_cols].values\n",
        "feature_names = feature_cols\n",
        "\n",
        "# Extract Targets (robust label extraction)\n",
        "if 'label' in df.columns:\n",
        "    y_raw = df['label'].values\n",
        "    print(\"Using 'label' column for targets\")\n",
        "elif 'label_code' in df.columns:\n",
        "    y_raw = df['label_code'].values\n",
        "    print(\"Using 'label_code' column for targets\")\n",
        "else:\n",
        "    # Fallback to filename extraction\n",
        "    print(\"Extracting labels from filenames...\")\n",
        "    y_raw = df['filename'].astype(str).str.extract(r'(?i)(h\\d+)')[0].str.upper().fillna('UNKNOWN').values\n",
        "\n",
        "print(f\"Number of species: {len(np.unique(y_raw))}\")\n",
        "print(f\"\\nSpecies distribution:\")\n",
        "print(pd.Series(y_raw).value_counts())\n",
        "\n",
        "# Handle any NaN values\n",
        "if np.isnan(X).any():\n",
        "    print(f\"\\nWarning: Found {np.isnan(X).sum()} NaN values. Filling with column means.\")\n",
        "    col_mean = np.nanmean(X, axis=0)\n",
        "    inds = np.where(np.isnan(X))\n",
        "    X[inds] = np.take(col_mean, inds[1])\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Number of features: {len(feature_names)}\")\n",
        "print(f\"Number of samples: {len(y_raw)}\")\n",
        "\n",
        "# Encode species labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_raw)\n",
        "species_names = le.classes_\n",
        "\n",
        "print(f\"\\nEncoded labels: {dict(zip(species_names, range(len(species_names))))}\")\n",
        "\n",
        "# Split data into train and test sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set size: {len(X_train)} samples\")\n",
        "print(f\"Test set size: {len(X_test)} samples\")\n",
        "print(f\"Train set distribution:\")\n",
        "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
        "for idx, count in train_dist.items():\n",
        "    print(f\"  {species_names[idx]}: {count}\")\n",
        "\n",
        "# Standardize features (fit on train, transform both)\n",
        "scaler_svm = StandardScaler()\n",
        "X_train_scaled = scaler_svm.fit_transform(X_train)\n",
        "X_test_scaled = scaler_svm.transform(X_test)\n",
        "\n",
        "print(\"\\n\u2713 Features standardized (zero mean, unit variance)\")\n",
        "print(\"\u2713 Data preparation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86608db7",
      "metadata": {},
      "source": [
        "## Step 2: Baseline SVM Models\n",
        "\n",
        "Train baseline SVM models with different kernels (linear, RBF, polynomial) to compare performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3387d4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: BASELINE SVM MODELS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test different SVM kernels\n",
        "kernels = ['linear', 'rbf', 'poly']\n",
        "baseline_results = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    print(f\"\\nTraining SVM with {kernel.upper()} kernel...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Create and train SVM\n",
        "    svm = SVC(kernel=kernel, random_state=42, gamma='scale')\n",
        "    svm.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predict on test set\n",
        "    y_pred = svm.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Store results\n",
        "    baseline_results[kernel] = {\n",
        "        'model': svm,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'training_time': training_time,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(f\"  Time:      {training_time:.2f}s\")\n",
        "\n",
        "# Compare baseline models\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"BASELINE MODEL COMPARISON\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Kernel': list(baseline_results.keys()),\n",
        "    'Accuracy': [baseline_results[k]['accuracy'] for k in baseline_results.keys()],\n",
        "    'Precision': [baseline_results[k]['precision'] for k in baseline_results.keys()],\n",
        "    'Recall': [baseline_results[k]['recall'] for k in baseline_results.keys()],\n",
        "    'F1-Score': [baseline_results[k]['f1'] for k in baseline_results.keys()],\n",
        "    'Time (s)': [baseline_results[k]['training_time'] for k in baseline_results.keys()]\n",
        "})\n",
        "\n",
        "display(comparison_df.round(4))\n",
        "\n",
        "# Visualize baseline comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot metrics\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "x = np.arange(len(kernels))\n",
        "width = 0.2\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    axes[0].bar(x + i*width, comparison_df[metric], width, label=metric)\n",
        "\n",
        "axes[0].set_xlabel('Kernel')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_title('Baseline Model Performance Comparison')\n",
        "axes[0].set_xticks(x + width * 1.5)\n",
        "axes[0].set_xticklabels([k.upper() for k in kernels])\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "axes[0].set_ylim([0, 1.1])\n",
        "\n",
        "# Plot training time\n",
        "axes[1].bar(kernels, comparison_df['Time (s)'])\n",
        "axes[1].set_xlabel('Kernel')\n",
        "axes[1].set_ylabel('Training Time (seconds)')\n",
        "axes[1].set_title('Training Time Comparison')\n",
        "axes[1].set_xticklabels([k.upper() for k in kernels])\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best baseline model\n",
        "best_kernel = max(baseline_results.keys(), key=lambda k: baseline_results[k]['accuracy'])\n",
        "print(f\"\\n\u2713 Best baseline kernel: {best_kernel.upper()} (Accuracy: {baseline_results[best_kernel]['accuracy']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "647e3f84",
      "metadata": {},
      "source": [
        "## Step 3: Hyperparameter Tuning\n",
        "\n",
        "Perform grid search with cross-validation to find optimal hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa860a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define parameter grid for RBF and Linear kernels\n",
        "param_grid = [\n",
        "    {\n",
        "        'kernel': ['rbf'],\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['linear'],\n",
        "        'C': [0.1, 1, 10, 100]\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['poly'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'degree': [2, 3, 4],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nParameter grid defined:\")\n",
        "print(f\"  RBF: C={[0.1, 1, 10, 100]}, gamma={['scale', 'auto', 0.001, 0.01, 0.1]}\")\n",
        "print(f\"  Linear: C={[0.1, 1, 10, 100]}\")\n",
        "print(f\"  Poly: C={[0.1, 1, 10]}, degree={[2, 3, 4]}, gamma={['scale', 'auto']}\")\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "print(\"\\nPerforming GridSearchCV (5-fold cross-validation)...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    SVC(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "grid_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n\u2713 Grid search completed in {grid_time:.2f}s\")\n",
        "print(f\"\\nBest parameters found:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get the best model\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# Test on held-out test set\n",
        "y_pred_best = best_svm.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Show top 10 parameter combinations\n",
        "print(\"\\nTop 10 parameter combinations:\")\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "top_results = results_df.nsmallest(10, 'rank_test_score')[\n",
        "    ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
        "]\n",
        "display(top_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad9eb01",
      "metadata": {},
      "source": [
        "## Step 4: Cross-Validation Analysis\n",
        "\n",
        "Perform detailed cross-validation to assess model stability and generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cc0933",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: CROSS-VALIDATION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Perform stratified k-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"\\nPerforming 10-fold stratified cross-validation...\")\n",
        "cv_scores = cross_val_score(best_svm, X_train_scaled, y_train, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(f\"\\nCross-validation scores (10 folds):\")\n",
        "for i, score in enumerate(cv_scores, 1):\n",
        "    print(f\"  Fold {i:2d}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\nCross-validation summary:\")\n",
        "print(f\"  Mean accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"  Median accuracy: {np.median(cv_scores):.4f}\")\n",
        "print(f\"  Min accuracy: {cv_scores.min():.4f}\")\n",
        "print(f\"  Max accuracy: {cv_scores.max():.4f}\")\n",
        "\n",
        "# Visualize cross-validation scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 11), cv_scores, 'o-', linewidth=2, markersize=8)\n",
        "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean: {cv_scores.mean():.4f}')\n",
        "plt.fill_between(range(1, 11), \n",
        "                 cv_scores.mean() - cv_scores.std(), \n",
        "                 cv_scores.mean() + cv_scores.std(), \n",
        "                 alpha=0.2, color='r', label=f'\u00b11 Std Dev')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('10-Fold Cross-Validation Scores')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim([max(0, cv_scores.min() - 0.05), min(1, cv_scores.max() + 0.05)])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2713 Cross-validation analysis complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ac3a6cb",
      "metadata": {},
      "source": [
        "## Step 5: Performance Evaluation\n",
        "\n",
        "Comprehensive evaluation with confusion matrix, classification report, and per-class metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8e62ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: PERFORMANCE EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get predictions on test set\n",
        "y_pred_final = best_svm.predict(X_test_scaled)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_final)\n",
        "precision = precision_score(y_test, y_pred_final, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred_final, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred_final, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\nOverall Performance Metrics:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"CLASSIFICATION REPORT (Per-Class Metrics)\")\n",
        "print(\"-\"*70)\n",
        "print(classification_report(y_test, y_pred_final, target_names=species_names, digits=4, zero_division=0))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# Count matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=species_names, yticklabels=species_names,\n",
        "            cbar_kws={'label': 'Count'}, ax=axes[0])\n",
        "axes[0].set_xlabel('Predicted Species')\n",
        "axes[0].set_ylabel('True Species')\n",
        "axes[0].set_title('Confusion Matrix (Counts)')\n",
        "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
        "axes[0].set_yticklabels(axes[0].get_yticklabels(), rotation=0)\n",
        "\n",
        "# Normalized confusion matrix (percentages)\n",
        "# Add small epsilon to avoid division by zero if a class has no samples in test set (though stratify helps)\n",
        "cm_sum = cm.sum(axis=1)[:, np.newaxis]\n",
        "cm_normalized = cm.astype('float') / (cm_sum + 1e-9)\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "            xticklabels=species_names, yticklabels=species_names,\n",
        "            cbar_kws={'label': 'Percentage'}, ax=axes[1])\n",
        "axes[1].set_xlabel('Predicted Species')\n",
        "axes[1].set_ylabel('True Species')\n",
        "axes[1].set_title('Normalized Confusion Matrix (Percentages)')\n",
        "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
        "axes[1].set_yticklabels(axes[1].get_yticklabels(), rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"\\nPer-Class Accuracy:\")\n",
        "for i, species in enumerate(species_names):\n",
        "    total = cm[i, :].sum()\n",
        "    class_accuracy = cm[i, i] / total if total > 0 else 0\n",
        "    print(f\"  {species}: {class_accuracy:.4f} ({cm[i, i]}/{total})\")\n",
        "\n",
        "print(\"\\n\u2713 Performance evaluation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d8eab2",
      "metadata": {},
      "source": [
        "## Step 6: Feature Importance Analysis\n",
        "\n",
        "Analyze which features contribute most to classification (available for linear kernel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d229a23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 6: FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if best_svm.kernel == 'linear':\n",
        "    print(\"\\nAnalyzing feature importance for linear SVM...\")\n",
        "    \n",
        "    # Get feature weights from linear SVM\n",
        "    # For multi-class, we have one weight vector per class\n",
        "    coef = best_svm.coef_\n",
        "    \n",
        "    print(f\"Coefficient matrix shape: {coef.shape}\")\n",
        "    print(f\"({coef.shape[0]} classes \u00d7 {coef.shape[1]} features)\")\n",
        "    \n",
        "    # Calculate mean absolute coefficient across all classes\n",
        "    mean_abs_coef = np.abs(coef).mean(axis=0)\n",
        "    \n",
        "    # Create feature importance dataframe\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': mean_abs_coef\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    display(feature_importance.head(20))\n",
        "    \n",
        "    # Visualize top 20 features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_20 = feature_importance.head(20)\n",
        "    plt.barh(range(len(top_20)), top_20['importance'])\n",
        "    plt.yticks(range(len(top_20)), top_20['feature'])\n",
        "    plt.xlabel('Mean Absolute Coefficient')\n",
        "    plt.title('Top 20 Most Important Features (Linear SVM)')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyze per-class feature importance\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"PER-CLASS FEATURE IMPORTANCE\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    n_classes = len(species_names)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_classes + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
        "    axes = axes.ravel() if n_classes > 1 else [axes]\n",
        "    \n",
        "    for i, species in enumerate(species_names):\n",
        "        if i < len(axes) and i < len(coef):\n",
        "            class_coef = np.abs(coef[i])\n",
        "            # Show top 5 features for NCA since we might have few features\n",
        "            top_k = min(10, len(feature_names))\n",
        "            top_features_idx = np.argsort(class_coef)[-top_k:][::-1]\n",
        "            \n",
        "            axes[i].barh(range(top_k), class_coef[top_features_idx])\n",
        "            axes[i].set_yticks(range(top_k))\n",
        "            axes[i].set_yticklabels([feature_names[j] for j in top_features_idx], fontsize=8)\n",
        "            axes[i].set_xlabel('Absolute Coefficient')\n",
        "            axes[i].set_title(f'{species}')\n",
        "            axes[i].invert_yaxis()\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(n_classes, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(f\"\\nFeature importance analysis not available for {best_svm.kernel} kernel\")\n",
        "    print(\"Use linear kernel to see feature weights\")\n",
        "    print(\"\\nNote: For non-linear kernels, feature importance is implicit in the support vectors\")\n",
        "    print(f\"Number of support vectors: {len(best_svm.support_)}\")\n",
        "    print(f\"Support vector ratio: {len(best_svm.support_) / len(X_train_scaled):.2%}\")\n",
        "\n",
        "print(\"\\n\u2713 Feature importance analysis complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3619164",
      "metadata": {},
      "source": [
        "## Step 7: Error Analysis\n",
        "\n",
        "Investigate misclassification patterns and decision confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26aebea5",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 7: ERROR ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified_mask = y_test != y_pred_final\n",
        "misclassified_indices = np.where(misclassified_mask)[0]\n",
        "n_misclassified = len(misclassified_indices)\n",
        "\n",
        "print(f\"\\nMisclassified samples: {n_misclassified}/{len(y_test)} ({n_misclassified/len(y_test)*100:.2f}%)\")\n",
        "\n",
        "if n_misclassified > 0:\n",
        "    # Analyze misclassification patterns\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"MISCLASSIFICATION PATTERNS\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    misclass_df = pd.DataFrame({\n",
        "        'True': [species_names[y_test[i]] for i in misclassified_indices],\n",
        "        'Predicted': [species_names[y_pred_final[i]] for i in misclassified_indices]\n",
        "    })\n",
        "    \n",
        "    # Count confusion pairs\n",
        "    confusion_pairs = misclass_df.groupby(['True', 'Predicted']).size().sort_values(ascending=False)\n",
        "    \n",
        "    print(\"\\nMost common misclassifications:\")\n",
        "    for (true_class, pred_class), count in confusion_pairs.head(10).items():\n",
        "        print(f\"  {true_class} \u2192 {pred_class}: {count} times\")\n",
        "    \n",
        "    # Visualize confusion pairs\n",
        "    if len(confusion_pairs) > 0:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        top_confusions = confusion_pairs.head(15)\n",
        "        labels = [f\"{true_cls} \u2192 {pred_cls}\" for (true_cls, pred_cls) in top_confusions.index]\n",
        "        plt.barh(range(len(top_confusions)), top_confusions.values)\n",
        "        plt.yticks(range(len(top_confusions)), labels)\n",
        "        plt.xlabel('Number of Misclassifications')\n",
        "        plt.title('Most Common Misclassification Patterns')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Decision confidence for misclassified samples\n",
        "    if hasattr(best_svm, 'decision_function'):\n",
        "        print(\"\\n\" + \"-\"*70)\n",
        "        print(\"DECISION CONFIDENCE ANALYSIS\")\n",
        "        print(\"-\"*70)\n",
        "        \n",
        "        # Get decision function values\n",
        "        decision_values = best_svm.decision_function(X_test_scaled)\n",
        "        \n",
        "        # For correctly classified samples\n",
        "        correct_mask = ~misclassified_mask\n",
        "        if decision_values.ndim > 1:\n",
        "            correct_confidence = np.max(decision_values[correct_mask], axis=1)\n",
        "            misclass_confidence = np.max(decision_values[misclassified_mask], axis=1)\n",
        "        else:\n",
        "            correct_confidence = np.abs(decision_values[correct_mask])\n",
        "            misclass_confidence = np.abs(decision_values[misclassified_mask])\n",
        "        \n",
        "        print(f\"\\nDecision confidence (mean absolute value):\")\n",
        "        print(f\"  Correct classifications: {np.mean(correct_confidence):.4f} \u00b1 {np.std(correct_confidence):.4f}\")\n",
        "        print(f\"  Misclassifications: {np.mean(misclass_confidence):.4f} \u00b1 {np.std(misclass_confidence):.4f}\")\n",
        "        \n",
        "        # Visualize confidence distribution\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(correct_confidence, bins=30, alpha=0.6, label='Correct', color='green')\n",
        "        plt.hist(misclass_confidence, bins=30, alpha=0.6, label='Misclassified', color='red')\n",
        "        plt.xlabel('Decision Function Confidence')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Decision Confidence Distribution')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"\\n\u2713 Perfect classification! No errors to analyze.\")\n",
        "\n",
        "print(\"\\n\u2713 Error analysis complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8baf5cc",
      "metadata": {},
      "source": [
        "## Step 8: Save Model and Generate Report\n",
        "\n",
        "Save the trained model and generate a comprehensive summary report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b3f238",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 8: SAVE MODEL AND GENERATE REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define filenames for NCA models\n",
        "if 'features_nca_10.csv' in features_file:\n",
        "    suffix = 'nca10'\n",
        "elif 'features_nca_2.csv' in features_file:\n",
        "    suffix = 'nca2'\n",
        "elif 'features_nca_9.csv' in features_file:\n",
        "    suffix = 'nca9'\n",
        "else:\n",
        "    suffix = 'nca'\n",
        "\n",
        "model_filename = f'../models/pola_{suffix}_svm_best.joblib'\n",
        "scaler_filename = f'../models/pola_{suffix}_scaler.joblib'\n",
        "label_encoder_filename = f'../models/pola_{suffix}_label_encoder.joblib'\n",
        "summary_filename = f'../reports/pola_{suffix}_svm_summary.json'\n",
        "\n",
        "joblib.dump(best_svm, model_filename)\n",
        "joblib.dump(scaler_svm, scaler_filename)\n",
        "joblib.dump(le, label_encoder_filename)\n",
        "\n",
        "print(f\"\\nSaved model artifacts:\")\n",
        "print(f\"  Model: {model_filename}\")\n",
        "print(f\"  Scaler: {scaler_filename}\")\n",
        "print(f\"  Label encoder: {label_encoder_filename}\")\n",
        "\n",
        "# Generate comprehensive summary report\n",
        "summary_report = {\n",
        "    'Model Type': f'Support Vector Machine (SVM) - {suffix.upper()}',\n",
        "    'Best Kernel': best_svm.kernel,\n",
        "    'Best Parameters': best_svm.get_params(),\n",
        "    'Training Samples': len(X_train),\n",
        "    'Test Samples': len(X_test),\n",
        "    'Number of Features': X.shape[1],\n",
        "    'Number of Classes': len(species_names),\n",
        "    'Class Names': list(species_names),\n",
        "    'Test Accuracy': f\"{accuracy:.4f}\",\n",
        "    'Test Precision': f\"{precision:.4f}\",\n",
        "    'Test Recall': f\"{recall:.4f}\",\n",
        "    'Test F1-Score': f\"{f1:.4f}\",\n",
        "    'CV Mean Accuracy': f\"{cv_scores.mean():.4f}\",\n",
        "    'CV Std Accuracy': f\"{cv_scores.std():.4f}\",\n",
        "    'Number of Support Vectors': len(best_svm.support_),\n",
        "    'Support Vector Ratio': f\"{len(best_svm.support_) / len(X_train_scaled):.2%}\",\n",
        "}\n",
        "\n",
        "# Save summary to JSON\n",
        "with open(summary_filename, 'w') as f:\n",
        "    json.dump(summary_report, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\nSaved model summary: {summary_filename}\")\n",
        "\n",
        "# Print final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SVM CLASSIFICATION PIPELINE - FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Model Configuration:':<30}\")\n",
        "print(f\"  {'Kernel:':<28} {best_svm.kernel}\")\n",
        "if best_svm.kernel in ['rbf', 'poly']:\n",
        "    print(f\"  {'C parameter:':<28} {best_svm.C}\")\n",
        "    print(f\"  {'Gamma:':<28} {best_svm.gamma}\")\n",
        "if best_svm.kernel == 'poly':\n",
        "    print(f\"  {'Degree:':<28} {best_svm.degree}\")\n",
        "elif best_svm.kernel == 'linear':\n",
        "    print(f\"  {'C parameter:':<28} {best_svm.C}\")\n",
        "\n",
        "print(f\"\\n{'Dataset Information:':<30}\")\n",
        "print(f\"  {'Total samples:':<28} {len(X)}\")\n",
        "print(f\"  {'Training samples:':<28} {len(X_train)}\")\n",
        "print(f\"  {'Test samples:':<28} {len(X_test)}\")\n",
        "print(f\"  {'Number of features:':<28} {X.shape[1]}\")\n",
        "print(f\"  {'Number of species:':<28} {len(species_names)}\")\n",
        "\n",
        "print(f\"\\n{'Performance Metrics:':<30}\")\n",
        "print(f\"  {'Test Accuracy:':<28} {accuracy:.4f}\")\n",
        "print(f\"  {'Test Precision:':<28} {precision:.4f}\")\n",
        "print(f\"  {'Test Recall:':<28} {recall:.4f}\")\n",
        "print(f\"  {'Test F1-Score:':<28} {f1:.4f}\")\n",
        "print(f\"  {'CV Accuracy (10-fold):':<28} {cv_scores.mean():.4f} \u00b1 {cv_scores.std():.4f}\")\n",
        "\n",
        "print(f\"\\n{'Model Characteristics:':<30}\")\n",
        "print(f\"  {'Support vectors:':<28} {len(best_svm.support_)}\")\n",
        "print(f\"  {'Support vector ratio:':<28} {len(best_svm.support_) / len(X_train_scaled):.2%}\")\n",
        "print(f\"  {'Misclassified (test):':<28} {n_misclassified}/{len(y_test)} ({n_misclassified/len(y_test)*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n{'Output Files:':<30}\")\n",
        "print(f\"  \u2022 {model_filename}\")\n",
        "print(f\"  \u2022 {scaler_filename}\")\n",
        "print(f\"  \u2022 {label_encoder_filename}\")\n",
        "print(f\"  \u2022 {summary_filename}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2713 SVM CLASSIFICATION PIPELINE COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nThe trained SVM model is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "supplementary_step",
      "metadata": {},
      "source": [
        "## Step 9: Supplementary - NCA2 Analysis\n",
        "\n",
        "Automatically process and generate a summary for the 2-dimensional NCA features (NCA2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "supplementary_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 9: SUPPLEMENTARY NCA2 ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "nca2_file = '../data/features_nca_2.csv'\n",
        "\n",
        "if os.path.exists(nca2_file):\n",
        "    print(f\"Processing {nca2_file}...\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    df_2 = pd.read_csv(nca2_file)\n",
        "    print(f\"Datasets Loaded: {df_2.shape}\")\n",
        "\n",
        "    # 2. Extract Features & Targets\n",
        "    feat_cols_2 = [c for c in df_2.columns if c.startswith('nca')]\n",
        "    X_2 = df_2[feat_cols_2].values\n",
        "\n",
        "    if 'label' in df_2.columns:\n",
        "        y_raw_2 = df_2['label'].values\n",
        "    elif 'label_code' in df_2.columns:\n",
        "        y_raw_2 = df_2['label_code'].values\n",
        "    else:\n",
        "        y_raw_2 = df_2['filename'].astype(str).str.extract(r'(?i)(h\\d+)')[0].str.upper().fillna('UNKNOWN').values\n",
        "\n",
        "    le_2 = LabelEncoder()\n",
        "    y_enc_2 = le_2.fit_transform(y_raw_2)\n",
        "\n",
        "    # 3. Split & Scale\n",
        "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
        "        X_2, y_enc_2, test_size=0.2, random_state=42, stratify=y_enc_2\n",
        "    )\n",
        "    scaler_2 = StandardScaler()\n",
        "    X_train_s2 = scaler_2.fit_transform(X_train_2)\n",
        "    X_test_s2 = scaler_2.transform(X_test_2)\n",
        "\n",
        "    # 4. Train Model (Using Linear SVM for this quick summary, or reusing known best params)\n",
        "    # We'll do a quick loop like baseline to pick best\n",
        "    best_acc = 0\n",
        "    best_m = None\n",
        "    best_k = ''\n",
        "\n",
        "    print(\"\\nTraining models for NCA2:\")\n",
        "    for k in ['linear', 'rbf', 'poly']:\n",
        "        m = SVC(kernel=k, random_state=42)\n",
        "        m.fit(X_train_s2, y_train_2)\n",
        "        acc = accuracy_score(y_test_2, m.predict(X_test_s2))\n",
        "        print(f\"  {k.upper()}: Accuracy={acc:.4f}\")\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_m = m\n",
        "            best_k = k\n",
        "\n",
        "    print(f\"\\nBest NCA2 model: {best_k.upper()} (Accuracy: {best_acc:.4f})\")\n",
        "\n",
        "    # 5. Save Artifacts\n",
        "    prefix = 'pola_nca2'\n",
        "    joblib.dump(best_m, f'../models/{prefix}_svm_best.joblib')\n",
        "    joblib.dump(scaler_2, f'../models/{prefix}_scaler.joblib')\n",
        "    joblib.dump(le_2, f'../models/{prefix}_label_encoder.joblib')\n",
        "\n",
        "    # 6. Save Summary JSON\n",
        "    y_pred_2 = best_m.predict(X_test_s2)\n",
        "    summ_2 = {\n",
        "        'Model Type': f'Support Vector Machine (SVM) - NCA2',\n",
        "        'Best Kernel': best_k,\n",
        "        'Test Accuracy': f\"{best_acc:.4f}\",\n",
        "        'Test Precision': f\"{precision_score(y_test_2, y_pred_2, average='weighted', zero_division=0):.4f}\",\n",
        "        'Test Recall': f\"{recall_score(y_test_2, y_pred_2, average='weighted', zero_division=0):.4f}\",\n",
        "        'Test F1-Score': f\"{f1_score(y_test_2, y_pred_2, average='weighted', zero_division=0):.4f}\"\n",
        "    }\n",
        "    with open(f'../reports/{prefix}_svm_summary.json', 'w') as f:\n",
        "        json.dump(summ_2, f, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved NCA2 summary to {prefix}_svm_summary.json\")\n",
        "    print(json.dumps(summ_2, indent=2))\n",
        "\n",
        "else:\n",
        "    print(\"\\nfeatures_nca_2.csv not found. Skipping supplementary analysis.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}